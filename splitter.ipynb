{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change paths to files in blocks 3 and 5 before use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all JSON files\n",
    "data_dir = \"/path/to/folder/with/json/files/\"\n",
    "json_files = [f for f in os.listdir(data_dir) if f.endswith(\".json\")]\n",
    "\n",
    "# Load data from JSON files and store as DataFrame\n",
    "data = []\n",
    "for filename in json_files:\n",
    "    with open(os.path.join(data_dir, filename), \"r\") as f:\n",
    "        data.append(json.load(f))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratsplit(df, column, proportions, shuffle=True, return_indices=False, check=False):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into n non-overlapping sets with specified proportions for a column. Handles some edge cases.\n",
    "\n",
    "    df: DataFrame to split (panda df)\n",
    "    column: Column to stratify by (str)\n",
    "    proportions: List of proportions for each set (list of floats <1 each), length equal to number of sets to split into\n",
    "    shuffle: Whether to shuffle the data after splitting (bool, default True)\n",
    "    return_indices: Whether to return the indices of the data points in the original df that were assigned to each set (bool, default False)\n",
    "    check: Whether to check if everything is consistent and provide numerical comparisions as proof (bool, default False)\n",
    "\n",
    "    Returns a list of DataFrames, one for each set.\n",
    "    \"\"\"\n",
    "    setcount = int(len(proportions))\n",
    "    \n",
    "    # For trying to split the df into 0 sets...\n",
    "    if setcount == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # For trying to split the df into 1 set...\n",
    "    if setcount == 1:\n",
    "        return df\n",
    "    \n",
    "    # Suggest alternative scikit-learn functions if the number of sets is 2\n",
    "    if setcount == 2:\n",
    "        print(\"Consider using train_test_split (not stratified by default), StratifiedShuffleSplit (potential overlap in test sets), or even StratifiedKFold from scikit-learn instead.\")\n",
    "    \n",
    "    # Check if proportions are valid for the number of sets\n",
    "    if sum([element * 100 for element in proportions]) != 100:\n",
    "        raise ValueError(\"Proportions list must be floats that sum to 1.\")\n",
    "\n",
    "    # Check if column name is a string\n",
    "    if not isinstance(column, str):\n",
    "        raise ValueError(\"Column name must be a string (enclosed in speech marks).\")\n",
    "    \n",
    "    # List of set sizes based on proportions (they will add up to len(df) because not division so no rounding inaccuracies)\n",
    "    set_sizes = [ int(prop * len(df)) for prop in proportions ] # ...needs to be adjusted if len(df) is small (otherwise eg. 19.417% of 70 is 13.59)\n",
    "\n",
    "    index_collection = [[] for _ in df[column].value_counts().index] # initialise a list of empty lists, one for each unique value in the stratify column\n",
    "    unique_proportions = [[] for _ in df[column].value_counts().index]\n",
    "\n",
    "    for j in range(len(df[column].value_counts().index)): # for each unique value in the stratify column\n",
    "        current = df[column].value_counts().index[j]\n",
    "        index_collection[j] = np.array(df[ df[column] == current ].index) # collect all of the data point indices (in df) for that current .index[] in a numpy array\n",
    "\n",
    "         # Proportion of data points with this unique value in stratify column\n",
    "        unique_proportions[j] = df.value_counts(column).values[j] / len(df)\n",
    "    \n",
    "    # For each set, calculate the number of data points for each unique value in the stratify column\n",
    "    unique_per_set = np.zeros((len(df[column].value_counts().index), setcount)) # initialise a 2D array of zeroes, with dimensions of the number of unique values in the stratify column and the number of sets\n",
    "    for i in range(setcount):\n",
    "        for j in range(len(df[column].value_counts().index)):\n",
    "            unique_per_set[j][i] = int(unique_proportions[j] * set_sizes[i])\n",
    "\n",
    "    # Adjust sizes of unique_per_set to match df[column].value_counts().values if needed (because most important is all data points are used without repetition, then proportions, THEN set_sizes are least important as they are flexible)\n",
    "    chosen_add, chosen_remove = [], [] # lists of indices of sets that have been previously edited and so should get less priority in future edits\n",
    "    for j in range(len(df[column].value_counts().index)):\n",
    "\n",
    "        while sum(unique_per_set[j,:]) < df.value_counts(column).values[j]: # i.e. there are data points (leftovers) with unique value [j] still to be assigned (explanation for code in > case down below) ...maybe they should be assigned cyclically through the sets but not sure how to choose starting point for each j (because if they all start from the same set and loop through the sets in the same order, then the first one will almost always be slightly bigger than necessary - try distributing 4 red candies, 9 blue candies, and 14 green candies into 5 groups while always starting from group 1 haha)\n",
    "            # ...problem is this deals with leftover data points one by one - maybe should add some way to check if the amount of leftovers for some unique value (from column) is divisible by number of sets, if yes then distribute some amount x equally to all before using the following code to distribute the remainder\n",
    "            prop_diff = [[] for _ in range(setcount)]\n",
    "            for i in range(setcount):\n",
    "                prop_diff[i] = ( unique_per_set[j,i] / sum(unique_per_set[:,i]) ) - unique_proportions[j] # ...could potentially set up something with abs(difference) for redistribution instead of just adding\n",
    "            min_index = [n for n, value in enumerate(prop_diff) if value == min(prop_diff)]\n",
    "            if len(min_index) == 1:\n",
    "                min_index = min_index[0]\n",
    "                if min_index not in chosen_add:\n",
    "                    chosen_add.append(min_index)\n",
    "            else:\n",
    "                min_index_temp = [k for k in min_index if k not in chosen_add]\n",
    "                if min_index_temp == []:\n",
    "                    chosen_add = [add_i for add_i in chosen_add if add_i not in min_index]\n",
    "                    min_index = random.choice(min_index)\n",
    "                    chosen_add.append(min_index)\n",
    "                else:\n",
    "                    min_index = random.choice(min_index_temp)\n",
    "                    chosen_add.append(min_index)\n",
    "            unique_per_set[j,min_index] += 1\n",
    "\n",
    "        # I actually think the following case should never happen because all sizes are calculated using some proportions and int(), which \"rounds\" down, but if the above while loop somehow messes with that, then hopefully this will fix things\n",
    "        while sum(unique_per_set[j,:]) > df.value_counts(column).values[j]:\n",
    "            # remove data point from the set where the local prop of data with unique value j is most different from the global prop of data with unique value j in df\n",
    "            prop_diff = [[] for _ in range(setcount)] # needs to be recalculated after every removal/addition and the while loops will keep going until equality\n",
    "            for i in range(setcount):\n",
    "                prop_diff[i] = ( unique_per_set[j,i] / sum(unique_per_set[:,i]) ) - unique_proportions[j]\n",
    "            max_index = [n for n, value in enumerate(prop_diff) if value == max(prop_diff)] # ...can potentially set up something to redistribute data points AFTER this\n",
    "            if len(max_index) == 1:\n",
    "                    max_index = max_index[0]\n",
    "                    if max_index not in chosen_remove:\n",
    "                        chosen_remove.append(max_index)\n",
    "            else: # ...but is this logical to do even if all prop_diff are negative (i.e. none have reached the desired proportions)?\n",
    "                max_index_temp = [k for k in max_index if k not in chosen_remove] # check the elements of max_index that are NOT in chosen_remove\n",
    "                if max_index_temp == []:\n",
    "                    chosen_remove = [rmv_i for rmv_i in chosen_remove if rmv_i not in max_index] # we remove ONLY the ones already in max_index\n",
    "                    max_index = random.choice(max_index)\n",
    "                    chosen_remove.append(max_index)\n",
    "                else:\n",
    "                    max_index = random.choice(max_index_temp)\n",
    "                    chosen_remove.append(max_index)\n",
    "            unique_per_set[j,max_index] -= 1\n",
    "        \n",
    "        if sum(unique_per_set[j,:]) == df.value_counts(column).values[j]:\n",
    "            break\n",
    "    \n",
    "    # ...could check if proportions in each set for the unique values are not too different from the global proportions, but how to reshuffle them if they aren't? maybe run for loop multiple times (different orders of j... but how to determine order?) and check proportions after for loop, then choose distribution with the smallest difference overall (how to optimise this?) from global proportions\n",
    "\n",
    "    # Shuffle the indices of data points in df with some unique value [j]\n",
    "    # faster to deal with indices than with the df itself, note: index_collection order is the same as df[column].value_counts().index order (which may be different to df.sort_values(by=column))\n",
    "    for j in range(len(df[column].value_counts().index)):\n",
    "        np.random.shuffle(index_collection[j])\n",
    "        np.random.shuffle(index_collection[j])\n",
    "    \n",
    "    # Create list of lists of indices to be assigned to each set\n",
    "    split_indices = [[] for _ in range(setcount)]\n",
    "\n",
    "    # Add unique_per_set[j,i] number of elements (the integer indices) from index_collection[j] to split_indices[i]\n",
    "    for i in range(setcount): # ...haven't checked if looping through j first (and accessing [j,:]) will be faster\n",
    "        for j in range(len(df[column].value_counts().index)):\n",
    "            split_indices[i].extend(index_collection[j][:int(unique_per_set[j,i])]) # neither append nor concatenate work (they changed element types from numpy.int64 to numpy.float64) ...why?\n",
    "            index_collection[j] = index_collection[j][int(unique_per_set[j,i]):] # delete the \"taken\" elements\n",
    "    \n",
    "    # Check length of sums of splits equals to len(df) i.e. if sizes are consistent ...but maybe better to check that every data point appears in split_indices and only once each\n",
    "    if sum([len(split_indices[i]) for i in range(setcount)]) != len(df):\n",
    "        raise ValueError(\"Splitting unsuccessful, check code.\") # ...could potentially return the original df instead of raising an error\n",
    "\n",
    "    # Shuffle the new dfs if shuffle is True\n",
    "    if shuffle:\n",
    "        for i in range(setcount):\n",
    "            np.random.shuffle(split_indices[i]) # ...can also shuffle the dfs using .sample(frac=1) but the indices will not be reset (\"in order\")\n",
    "    \n",
    "    # Create list of new dfs (the split sets)\n",
    "    split = [pd.DataFrame() for _ in range(setcount)]\n",
    "\n",
    "    # Assign data points corresponding to the indices in split_indices to the new dfs\n",
    "    for i in range(setcount):\n",
    "        split[i] = df.loc[split_indices[i],:]\n",
    "        if shuffle:\n",
    "            split[i].reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if check:\n",
    "        checklist = [] # ...couldn't get it to print \\n using vscode\n",
    "\n",
    "        # DF length consistency check\n",
    "        if sum([len(s) for s in split]) != len(df):\n",
    "            checklist.append(f\"The lengths are NOT consistent; the length of df is {len(df)} and the sum of the lengths of the split sets is {sum([len(s) for s in split])}. \")\n",
    "        else:\n",
    "            checklist.append(f\"The lengths ARE consistent; both df and the sum of the lengths of the split sets have {len(df)} data points. \")\n",
    "\n",
    "        # Stratify column length consistency check\n",
    "        sumcheck = [0 for _ in range(df.value_counts(column).index.size)]\n",
    "        for i in range(setcount):\n",
    "            for j in range(df.value_counts(column).index.size): # sumcheck will use the order of .index in the original df\n",
    "                try:\n",
    "                    j_split = split[i].value_counts(column).index.get_loc(df.value_counts(column).index[j])\n",
    "                except ValueError:\n",
    "                    checklist.append(f\"The category {df.value_counts(column).index[j]} is not present in set {i}. \")\n",
    "                    break\n",
    "                try:\n",
    "                    sumcheck[j_split] += split[i].value_counts(column).values[j]\n",
    "                except ValueError:\n",
    "                    checklist.append(f\"There are less unique values from {column} in split set {i} than in the original data set. \")\n",
    "                    break\n",
    "        for j in range(len(sumcheck)):\n",
    "            # note: now j in here is the correct index for both sumcheck and df.value_counts(column).index\n",
    "            try:\n",
    "                if sumcheck[j] != df.value_counts(column).values[j]:\n",
    "                    checklist.append(f\"There is an inconsistency in the stratify column {column}; there are a total of {sumcheck[j]} data points in the split sets that belong to category {df.value_counts(column).index[j]}, while there are {df.value_counts(column).values[j]} in the original data set. \")\n",
    "                else:\n",
    "                    checklist.append(f\"The category {df.value_counts(column).index[j]} is consistent; there are {sumcheck[j]} data points in both the original data set and totalled across all of the split data sets. \")\n",
    "            except ValueError:\n",
    "                checklist.append(f\"There was a problem summing up data points for category {df.value_counts(column).index[j]}. \")\n",
    "                break\n",
    "\n",
    "        # Proportion consistency check\n",
    "        for i in range(setcount):\n",
    "            div = len(split[i])/len(df)\n",
    "            if abs( div - proportions[i] ) > 0.09: # ...what is a good value to use here?\n",
    "                checklist.append(f\"The proportion of set {i} is inconsistent; it is {div} when it should be {proportions[i]}. \")\n",
    "            else:\n",
    "                checklist.append(f\"The proportion of set {i} is consistent; it is {div}, which is within 0.1 of the assigned {proportions[i]} proportion. \")\n",
    "\n",
    "        # Proportion consistency check within split sets\n",
    "        unique_splitted = np.zeros((len(df[column].value_counts().index), setcount))\n",
    "        for i in range(setcount):\n",
    "            for j in range(df[column].value_counts().index.size):\n",
    "                unique_splitted[j][i] = split[i].value_counts(column).values[j] / len(split[i])\n",
    "        message = f\"The stratify column is \\\"{column}\\\". \"\n",
    "        for j in range(df[column].value_counts().index.size):\n",
    "            message += f\"For {df[column].value_counts().index[j]}, the original proportions are {unique_proportions[j]} and in the {setcount} split sets, they are {unique_splitted[j,:]} respectively. \"\n",
    "        checklist.append(message)\n",
    "\n",
    "        # ...add some way to check if each data point from the original df only appears once across all split sets (maybe use indices list)\n",
    "\n",
    "        if return_indices:\n",
    "            return split, split_indices, checklist\n",
    "        \n",
    "        return split, checklist\n",
    "    \n",
    "    if return_indices:\n",
    "        return split, split_indices\n",
    "\n",
    "    return split\n",
    "\n",
    "\n",
    "# Function to write JSON objects from a DataFrame as JSONL format\n",
    "def write_jsonl(df, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        for index, row in df.iterrows():\n",
    "            row_dict = row.to_dict() # convert rows to dictionaries\n",
    "            f.write(json.dumps(row_dict) + \"\\n\")\n",
    "\n",
    "\n",
    "# Function to read jsonl files and convert them into dataframes just to check the split datasets to jsonl files function worked\n",
    "def read_jsonl(pathtofile):\n",
    "  filedata = []\n",
    "  with open(pathtofile, \"r\") as f:\n",
    "    for line in f:\n",
    "      if not line.strip(): # skips empty lines\n",
    "        continue\n",
    "      filedata.append(json.loads(line))\n",
    "  return pd.DataFrame(filedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the splitter\n",
    "split, checklist = stratsplit(df, \"topic;\", [0.7, 0.2, 0.1], shuffle=True, return_indices=False, check=True)\n",
    "print(checklist)\n",
    "\n",
    "# Write the split sets to JSONL files\n",
    "write_jsonl(split[0], \"/path/to/train/file.jsonl\")\n",
    "write_jsonl(split[1], \"/path/to/test/file.jsonl\")\n",
    "write_jsonl(split[2], \"/path/to/val/file.jsonl\") # least important, could/should(?) actually be taken from within training set\n",
    "\n",
    "# Import one back to check if it worked\n",
    "#df_test = read_jsonl(\"/path/to/train/file.jsonl\")\n",
    "#len(df_test)\n",
    "#df_test.value_counts(\"topic;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redistribution idea:\n",
    "* if, for some j, we have sum(unique_per_set[j,:]) > df.value_counts(column).values[j], then check if there are instances of sum(unique_per_set[other_j,:]) < df.value_counts(column).values[other_j]\n",
    "    * if there is, then store other_j (can be more than one)\n",
    "    * idea is to -1 from unique_per_set[j,:] and +1 to unique_per_set[other_j,:]\n",
    "    * if this is done, need to consider set_sizes\n",
    "        * ideally redistribute within same set, but next best case is to remove from a set that hasn't had data points removed from before AND doesn't have perfect proportions, and add to a set that hasn't had data points added to before AND doesn't have perfect proportions\n",
    "* determining where data should be added to or removed from, need to calculate proportions of all unique values (j) for all sets (i)\n",
    "    * prop[j][i] = unique_per_set[j,i] / set_sizes[i]\n",
    "    * should also consider prop[j][i] - unique_proportions[j]\n",
    "        * a greater value (either the negative closest to 0 or the biggest positive) is ideal to take from, a smaller value is idea to give to, a value close to 0 should mean that set will remain untouched unless if all sets are like that\n",
    "* ideally, redistribution would be from unique value j to unique value other_j, from set with greater difference value to the set with the smallest\n",
    "    * if there are multiple sets with greatest (or smallest) difference value, then randomise to choose one\n",
    "    * not sure if them being the same set will cause problems or not\n",
    "    * if only splitting into 2 sets and somehow both have same differences from unique_proportions, then is it better to redistribute from one to the other or redistribute within same set?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
